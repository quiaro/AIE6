### ‚ùì Question #1:

What are the three types of query synthesizers doing? Describe each one in simple terms.

**SingleHopSpecificQuerySynthesizer**: generates simple, direct questions that can be answered by retrieving information from a single source or node in the knowledge graph.

**MultiHopSpecificQuerySynthesizer**: generates questions that require connecting multiple pieces of factual information from the knowledge graph.

**MultiHopAbstractQuerySynthesizer**: generates complex questions that require combining concepts rather than specific facts through multiple "hops" or connections in the knowledge graph.

### üèóÔ∏è Activity #2:

Highlight what each evaluator is evaluating.

- `qa_evaluator`: Measures the correctness of the response to a user query by comparing it to a reference answer.
- `labeled_helpfulness_evaluator`: Measures how helpful a response is to a user taking into account the correct reference answer.
- `dope_or_nope_evaluator`: Measures if the response provided by the app can be deemed dope, lit or cool.

### ‚ùìQuestion #2:

Why would modifying our chunk size modify the performance of our application?

Increasing the chunk size provides each chunk with more context thereby increasing the likelihood of finding complete and correct responses to the queries. However, if the chunk sizes are too large, then this may negatively impact retrieval since the chunks would have so much information that it would become more difficult to detect their relevance to the query.

### ‚ùìQuestion #3:

Why would modifying our embedding model modify the performance of our application?

The embeddings model `"text-embedding-3-large"` has a higher embedding vector dimension (3072). This way, after increasing the chunk size, **we guarantee that the entire content of each chunk is correctly represented as a vector**. However, it may be useful to use the `dimensions` parameter to adjust the embedding dimension to be closer to the length of the chunks so that the embeddings are more semantically rich and there are less computational and storage costs.

### üèóÔ∏è Activity #3:

Provide a screenshot of the difference between the two chains, and explain why you believe certain metrics changed in certain ways.

![Chain comparison](./images/experiments.png 'Chain Experiments - Comparison')

- **Correctness**: The dopeness added to the responses may have caused the evaluator to penalize the correctness since the references generated by the TestsetGenerator are not all that dope. In order to fix this, we could create a custom evaluator that evaluates for correctness differently or we could generate a dataset using a persona that can provide answers that are dope and then run the tests against this other dataset.

- **Dopeness**: The dopeness evaluator detected the change in dopeness in the answers provided by the `dope_rag_chain`.

- **Helpfulness**: Similar to the correctness evaluator, this custom evaluator compares the responses from the `dope_rag_chain` with references in the generated dataset that are not dope. This may have negatively impacted the helpfulness score.
