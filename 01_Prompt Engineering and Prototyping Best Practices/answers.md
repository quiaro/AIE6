##### ğŸ—ï¸ Activity #1:

Please evaluate your system on the following questions:

### 1. Explain the concept of object-oriented programming in simple terms to a complete beginner.

- Aspect Tested: **Accuracy: Good**. The information provided by the model is correct.
- Aspect Tested: **Relevance: Good**. The information provided by the model is on topic and appropriate for the target audience (beginner programmers).
- Aspect Tested: **Clarity: Good**. The information provided by the model uses simple terms and is easy to understand.
- Aspect Tested: **Conciseness: Good**. The explanation provided by the model is brief and to the point (less than 10 sentences long).
- Aspect Tested: **Coherency: Good**. The explanation provided by the model is logical and well-structured.

### 2. Read the following paragraph and provide a concise summary of the key points.

_Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions â€“ something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general._

- Aspect Tested: **Accuracy: Good**. The information provided by the model is correct.
- Aspect Tested: **Relevance: Good**. The information provided by the model is on topic.
- Aspect Tested: **Clarity: Good**. The information provided by the model is easy to understand.
- Aspect Tested: **Conciseness: Good**. The model does a good job in synthesizing the paragraph.
- Aspect Tested: **Coherency: Good**. The information provided by the model is well-structured.

### 3. Write a short, imaginative story (100â€“150 words) about a robot finding friendship in an unexpected place.

- Aspect Tested: **Accuracy: Excellent**. The information provided by the model is correct.
- Aspect Tested: **Relevance: Excellent**. The information provided by the model is on topic.
- Aspect Tested: **Clarity: Excellent**. The story provided by the model is easy to understand.
- Aspect Tested: **Coherency: Excellent**. The story provided by the model follows a logical sequence.

### 4. If a store sells apples in packs of 4 and oranges in packs of 3, how many packs of each do I need to buy to get exactly 12 apples and 9 oranges?

- Aspect Tested: **Accuracy: Excellent**. The information provided by the model is correct.
- Aspect Tested: **Relevance: Excellent**. The information provided by the model is on topic.
- Aspect Tested: **Clarity: Excellent**. The explanation provided by the model is easy to understand.
- Aspect Tested: **Coherency: Excellent**. The explanation provided by the model is logical and well-structured.

### 5. Rewrite the following paragraph in a professional, formal tone.

_ğŸ‰ğŸˆğŸ‚ Hey there, my awesome friends! ğŸ‚ğŸˆğŸ‰_

_I hope this message finds you all well and excited because it's that time of the year again - my birthday is coming up! ğŸ¥³ğŸ‰_

_I would be absolutely thrilled if you could join me for a fun-filled celebration at my place. We'll be serving delicious quesadillas and refreshing pop to satisfy our taste buds. ğŸŒ®ğŸ¥¤_

_After enjoying some yummy snacks, we can dive into some friendly competition with board games or cozy up for a movie night with selections from Netflix. ğŸ²ğŸ¥_

_So mark your calendars and save the date! Let's make some unforgettable memories together on April 8th at 22 Flinn Drive. The party starts at 6pm. ğŸ‰ğŸˆ_

_See you soon! ğŸ¥³âœ¨_

- Aspect Tested: **Accuracy: Excellent**. The information provided by the model is correct.
- Aspect Tested: **Relevance: Excellent**. The information provided by the model is on topic.
- Aspect Tested: **Clarity: Excellent**. The information provided by the model is easy to understand.
- Aspect Tested: **Coherency: Excellent**. The information provided by the model is consistent with the prompt.

##### ğŸš§ Advanced Build:

Please make adjustments to your application that you believe will improve the vibe check done above, push the changes to your HF Space and redo the above vibe check.

### 1. Explain the concept of object-oriented programming in simple terms to a complete beginner.

- Aspect Tested: **Accuracy: Excellent**. The information provided by the model is correct.
- Aspect Tested: **Relevance: Excellent**. The information provided by the model is on topic.
- Aspect Tested: **Clarity: Excellent**. The information provided by the model is easy to understand and it is outlined clearly.
- Aspect Tested: **Conciseness: Failed**. The explanation provided by the model is too long and gets cut off.
- Aspect Tested: **Coherency: Excellent**. The explanation provided by the model is logical and well-structured.

### 2. Read the following paragraph and provide a concise summary of the key pointsâ€¦

_Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions â€“ something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general._

- Aspect Tested: **Accuracy: Room for Improvement**. The summary provided by the model is repetitive and not entirely correct.
- Aspect Tested: **Relevance: Good**. The summary provided by the model is related completely to the paragraph.
- Aspect Tested: **Clarity: Excellent**. The numbering and usage of bullet points to list the key points makes the summary easier to understand.
- Aspect Tested: **Conciseness: Room for Improvement**. The summary of the model is extensive and it's almost as long as the original paragraph.
- Aspect Tested: **Coherency: Good**. The information provided by the model follows a logical sequence per the original paragraph.

### 3. Write a short, imaginative story (100â€“150 words) about a robot finding friendship in an unexpected place.

- Aspect Tested: **Accuracy: Excellent**. The information provided by the model is correct.
- Aspect Tested: **Relevance: Excellent**. The information provided by the model is on topic.
- Aspect Tested: **Clarity: Excellent**. The story provided by the model is easy to understand.
- Aspect Tested: **Coherency: Excellent**. The story provided by the model follows a logical sequence.

### 4. If a store sells apples in packs of 4 and oranges in packs of 3, how many packs of each do I need to buy to get exactly 12 apples and 9 oranges?

- Aspect Tested: **Accuracy: Excellent**. The information provided by the model is correct.
- Aspect Tested: **Relevance: Good**. The information provided by the model is on topic, though a bit verbose.
- Aspect Tested: **Clarity: Excellent**. The explanation provided by the model is easy to understand. It also helps that it's formatted using LaTeX.
- Aspect Tested: **Coherency: Excellent**. The explanation provided by the model is logical and well-structured.

### 5. Rewrite the following paragraph in a professional, formal tone.

_ğŸ‰ğŸˆğŸ‚ Hey there, my awesome friends! ğŸ‚ğŸˆğŸ‰_

_I hope this message finds you all well and excited because it's that time of the year again - my birthday is coming up! ğŸ¥³ğŸ‰_

_I would be absolutely thrilled if you could join me for a fun-filled celebration at my place. We'll be serving delicious quesadillas and refreshing pop to satisfy our taste buds. ğŸŒ®ğŸ¥¤_

_After enjoying some yummy snacks, we can dive into some friendly competition with board games or cozy up for a movie night with selections from Netflix. ğŸ²ğŸ¥_

_So mark your calendars and save the date! Let's make some unforgettable memories together on April 8th at 22 Flinn Drive. The party starts at 6pm. ğŸ‰ğŸˆ_

_See you soon! ğŸ¥³âœ¨_

- Aspect Tested: **Accuracy: Excellent**. The information provided by the model is correct.
- Aspect Tested: **Relevance: Excellent**. The information provided by the model is on topic.
- Aspect Tested: **Clarity: Excellent**. The information provided by the model is easy to understand.
- Aspect Tested: **Coherency: Excellent**. The translation provided by the model is consistent with the prompt.

##### ğŸ§‘â€ğŸ¤â€ğŸ§‘â“ Discussion Question #1:

What are some limitations of vibe checking as an evaluation tool?

"Vibe checking" refers to an informal and non-comprehensive evaluation of LLM-powered systems, which poses challenges for scalability, consistency, and progress tracking. As these systems grow, manual testing becomes increasingly slow and time-consuming. Additionally, inconsistent test coverage may lead to inaccurate assessments of the system's status. Finally, vibe checking lacks a reliable baseline for measuring the impact of future changes.
